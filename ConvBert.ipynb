{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alsol\\miniconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\alsol\\miniconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "c:\\Users\\alsol\\miniconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv('dataset/train_test_4class_no_empty_fixed_helpful_lower_punch_stopwords_emotion_outliers_eng.csv', encoding='latin-1', na_filter=True)\n",
    "\n",
    "\n",
    "columns_to_drop = ['OverallScore']\n",
    "data.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "\n",
    "data.dropna(subset=['label'], inplace=True)\n",
    "\n",
    "\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_data, rest_data = train_test_split(data, test_size=0.2, random_state=42, stratify=data['label'])\n",
    "\n",
    "\n",
    "validation_data, test_data = train_test_split(rest_data, test_size=0.5, random_state=42, stratify=rest_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "3    3197\n",
      "2     667\n",
      "4     518\n",
      "1     198\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class_support_train = train_data['label'].value_counts()\n",
    "print(class_support_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "3    399\n",
      "2     83\n",
      "4     65\n",
      "1     25\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class_support_validation = validation_data['label'].value_counts()\n",
    "print(class_support_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "3    400\n",
      "2     84\n",
      "4     65\n",
      "1     24\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class_support_test = test_data['label'].value_counts()\n",
    "print(class_support_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "validation_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5364\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label = LabelEncoder()\n",
    "y_train = label.fit_transform(train_data['label'])\n",
    "y_train = to_categorical(y_train)\n",
    "y_validation = label.transform(validation_data['label'])\n",
    "y_validation = to_categorical(y_validation)\n",
    "y_test = label.transform(test_data['label'])\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5210\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFConvBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"YituTech/conv-bert-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFConvBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFConvBertForSequenceClassification were not initialized from the model checkpoint at YituTech/conv-bert-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Changed the tokenizer to ALBERT\n",
    "# tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "\n",
    "convBert_model_layer = TFConvBertForSequenceClassification.from_pretrained(\"YituTech/conv-bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial One\n",
    "\n",
    "def convBert_encode(data, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        text = (str(data['ReviewerName'][i]) + \" \" +\n",
    "                str(data['ProductTitle'][i]) + \" \" +\n",
    "                str(data['ProductDetails'][i]) + \" \" +\n",
    "                str(data['ReviewTitle'][i]) + \" \" +\n",
    "                str(data['ReviewDetails'][i]))\n",
    "        \n",
    "        numerical_columns = [str(data[column][i]) for column in ['ReviewRating', \n",
    "                                                                 'VerifiedPurchase', 'HelpfulVote',\n",
    "                                                                 'ImageStatus', 'VineReviewStatus']]\n",
    "        \n",
    "        text = tokenizer.tokenize(text) + numerical_columns\n",
    "        \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "        \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convBert_encode(data, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        text = (str(data['ReviewerName'][i]) + \" \" +\n",
    "                str(data['ProductTitle'][i]) + \" \" +\n",
    "                str(data['ProductDetails'][i]) + \" \" +\n",
    "                str(data['ReviewTitle'][i]) + \" \" +\n",
    "                str(data['ReviewDetails'][i]))\n",
    "        \n",
    "        numerical_columns = [str(data[column][i]) for column in ['ReviewRating', \n",
    "                                                                 'VerifiedPurchase', 'HelpfulVote',\n",
    "                                                                 'ImageStatus', 'VineReviewStatus']]\n",
    "        \n",
    "        text = text + \" \" + \" \".join(numerical_columns)\n",
    "        print(text)\n",
    "        \n",
    "        inputs = tokenizer.encode_plus(text, add_special_tokens=True, max_length=max_len, \n",
    "                                       padding='max_length', truncation=True)\n",
    "        \n",
    "        all_tokens.append(inputs['input_ids'])\n",
    "        all_masks.append(inputs['attention_mask'])\n",
    "        all_segments.append(inputs['token_type_ids'])\n",
    "        \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 250\n",
    "albert_train_input = convBert_encode(train_data, tokenizer, max_len=max_len)\n",
    "albert_validation_input = convBert_encode(validation_data, tokenizer, max_len=max_len)\n",
    "albert_test_input = convBert_encode(test_data, tokenizer, max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked bidirectional LSTM with both sequence and pooled outputs\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "def build_convBert_model(convBert_model,  max_len=250):\n",
    "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "    \n",
    "\n",
    "    convBert_outputs = convBert_model(input_word_ids, attention_mask=input_mask, token_type_ids=segment_ids)\n",
    "    convBert_sequence_output = convBert_outputs[0]\n",
    "    convBert_sequence_output = tf.tile(tf.expand_dims(convBert_sequence_output, axis=1), [1, 512, 1])\n",
    "    \n",
    "\n",
    "    lstm_output = Bidirectional(LSTM(256, return_sequences=True))(convBert_sequence_output)\n",
    "    lstm_output = Bidirectional(LSTM(256, return_sequences=True))(lstm_output)\n",
    "    lstm_output = Bidirectional(LSTM(256))(lstm_output)\n",
    "    \n",
    "    lay = Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(lstm_output)\n",
    "    lay = Dropout(0.2)(lay) # make dropout layer 0.5 to decrease the train accuracy\n",
    "    lay = Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(lay)\n",
    "    lay = Dropout(0.2)(lay)\n",
    "    lay = Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(lay)\n",
    "    lay = Dropout(0.2)(lay)\n",
    "    out = Dense(3, activation='softmax', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(lay)\n",
    "\n",
    "    # lay = tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))(lstm_output)\n",
    "    # lay = tf.keras.layers.Dropout(0.2)(lay)\n",
    "    # lay = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))(lay)\n",
    "    # lay = tf.keras.layers.Dropout(0.2)(lay)\n",
    "    # lay = tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))(lay)\n",
    "    # lay = tf.keras.layers.Dropout(0.2)(lay)\n",
    "    # out = tf.keras.layers.Dense(4, activation='softmax', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))(lay)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(albert_train_input[0]) == len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_word_ids (InputLayer)    [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_mask (InputLayer)        [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " segment_ids (InputLayer)       [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_conv_bert_for_sequence_clas  TFSequenceClassifie  106272650  ['input_word_ids[0][0]',         \n",
      " sification (TFConvBertForSeque  rOutput(loss=None,               'input_mask[0][0]',             \n",
      " nceClassification)             logits=(None, 2),                 'segment_ids[0][0]']            \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda)    (None, 1, 2)         0           ['tf_conv_bert_for_sequence_class\n",
      "                                                                 ification[0][0]']                \n",
      "                                                                                                  \n",
      " tf.tile (TFOpLambda)           (None, 512, 2)       0           ['tf.expand_dims[0][0]']         \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 512, 512)     530432      ['tf.tile[0][0]']                \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 512, 512)    1574912     ['bidirectional[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  (None, 512)         1574912     ['bidirectional_1[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          65664       ['bidirectional_2[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           8256        ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_39 (Dropout)           (None, 64)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           2080        ['dropout_39[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_40 (Dropout)           (None, 32)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 3)            99          ['dropout_40[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 110,029,005\n",
      "Trainable params: 110,029,005\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "convBert_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convBert_encode_text_only(data, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    all_numerical_data = []\n",
    "                    # str(data['ProfileID'][i]) + \" \" +\n",
    "    for i in range(len(data)): # keeping ReviewDetails at first so even if it gets Truncated, ReviewDetails will still the there at most\n",
    "        text = (str(data['ReviewDetails'][i]) + \" \" +\n",
    "                str(data['ProductDetails'][i]) + \" \" +\n",
    "                str(data['ProductTitle'][i]) + \" \" +\n",
    "                str(data['ReviewTitle'][i]) + \" \" +\n",
    "                str(data['Date'][i]) + \" \" +\n",
    "                str(data['ReviewerName'][i]))\n",
    "\n",
    "        \n",
    "        numerical_data = [data[column][i] for column in ['ReviewRating', 'HelpfulVote', 'ProductPrice',\n",
    "                                                         'VerifiedPurchase',\n",
    "                                                         'ImageStatus', 'VineReviewStatus']]\n",
    "        \n",
    "        inputs = tokenizer.encode_plus(text, add_special_tokens=True, max_length=max_len, \n",
    "                                       padding='max_length', truncation=True)\n",
    "        \n",
    "        all_tokens.append(inputs['input_ids'])\n",
    "        all_masks.append(inputs['attention_mask'])\n",
    "        all_segments.append(inputs['token_type_ids'])\n",
    "        all_numerical_data.append(numerical_data)\n",
    "        \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments), np.array(all_numerical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.reset_index(drop=True, inplace=True)\n",
    "validation_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 512\n",
    "# albert_train_input = convBert_encode_text_only(train_data, tokenizer, max_len=max_len)\n",
    "albert_train_input = convBert_encode_text_only(train_data, tokenizer, max_len=max_len)\n",
    "albert_validation_input = convBert_encode_text_only(validation_data, tokenizer, max_len=max_len)\n",
    "albert_test_input = convBert_encode_text_only(test_data, tokenizer, max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM\n",
    "\n",
    "# robust model with many columns and numerical data\n",
    "\n",
    "def build_convBert_model_kfolds(convBert_model, num_numerical_features, max_len=250):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "    numerical_input = Input(shape=(num_numerical_features,), name=\"numerical_input\")\n",
    "    \n",
    "\n",
    "    convBert_outputs = convBert_model(input_word_ids, attention_mask=input_mask, token_type_ids=segment_ids)\n",
    "    convBert_sequence_output = convBert_outputs[0]\n",
    "    convBert_sequence_output = tf.tile(tf.expand_dims(convBert_sequence_output, axis=1), [1, 512, 1])\n",
    "    \n",
    "\n",
    "    lstm_output = Bidirectional(LSTM(256, return_sequences=True))(convBert_sequence_output)\n",
    "    lstm_output = Bidirectional(LSTM(256, return_sequences=True))(lstm_output)\n",
    "    lstm_output = Bidirectional(LSTM(256))(lstm_output)\n",
    "    \n",
    "\n",
    "    numerical_output = Dense(32, activation='relu')(numerical_input)\n",
    "    numerical_output = Dropout(0.2)(numerical_output)\n",
    "    \n",
    "\n",
    "    concatenated = concatenate([lstm_output, numerical_output])\n",
    "    \n",
    "    lay = Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(concatenated)\n",
    "    lay = Dropout(0.2)(lay)\n",
    "    lay = Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(lay)\n",
    "    lay = Dropout(0.2)(lay)\n",
    "    lay = Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(lay)\n",
    "    lay = Dropout(0.2)(lay)\n",
    "    out = Dense(3, activation='softmax', kernel_regularizer=l1_l2(l1=0.01, l2=0.01))(lay)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids, numerical_input], outputs=out)\n",
    "    model.compile(Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main convBert Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFConvBertModel.\n",
      "\n",
      "All the layers of TFConvBertModel were initialized from the model checkpoint at YituTech/conv-bert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFConvBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModel\n",
    "convBert_model_layer = TFAutoModel.from_pretrained('YituTech/conv-bert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concating Numerical with LSTM by changing numerical shape, the main now model\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Bidirectional, LSTM, GlobalAveragePooling1D, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_end_to_end_convBert_model_main(convBert_model, num_numerical_features, max_len=512):\n",
    "\n",
    "\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "    numerical_input = Input(shape=(num_numerical_features,), name=\"numerical_input\")\n",
    "\n",
    "\n",
    "    convBert_outputs = convBert_model(input_word_ids, attention_mask=input_mask, token_type_ids=segment_ids)\n",
    "    convBert_sequence_output = convBert_outputs[0]\n",
    "\n",
    "\n",
    "    numerical_output = Dense(max_len, activation='relu')(numerical_input)\n",
    "    numerical_output = Dropout(0.2)(numerical_output)\n",
    "\n",
    "\n",
    "    numerical_output = tf.expand_dims(numerical_output, 1)\n",
    "\n",
    "\n",
    "    numerical_output = tf.tile(numerical_output, [1, max_len, 1])\n",
    "\n",
    "\n",
    "    concatenated = concatenate([convBert_sequence_output, numerical_output], axis=-1)\n",
    "\n",
    "    lstm_output = Bidirectional(LSTM(256, return_sequences=True))(concatenated)\n",
    "    lstm_output = Bidirectional(LSTM(128))(lstm_output)\n",
    "\n",
    "\n",
    "    lay = Dense(128, activation='relu')(lstm_output)\n",
    "    lay = Dropout(0.2)(lay)\n",
    "    out = Dense(4, activation='softmax')(lay)\n",
    "    # 0.00001 learning rate had fluctuation, so change it to 0.000001\n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids, numerical_input], outputs=out)\n",
    "    model.compile(Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_numerical_features = 6\n",
    "max_len = 512\n",
    "# convBert_model = build_convBert_model(model, num_numerical_features, max_len=max_len)\n",
    "convBert_model = build_end_to_end_convBert_model_main(convBert_model_layer, num_numerical_features, max_len=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " numerical_input (InputLayer)   [(None, 6)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 512)          3584        ['numerical_input[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_77 (Dropout)           (None, 512)          0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " input_word_ids (InputLayer)    [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_mask (InputLayer)        [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " segment_ids (InputLayer)       [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf.expand_dims_2 (TFOpLambda)  (None, 1, 512)       0           ['dropout_77[0][0]']             \n",
      "                                                                                                  \n",
      " tf_conv_bert_model (TFConvBert  TFBaseModelOutput(l  105680520  ['input_word_ids[0][0]',         \n",
      " Model)                         ast_hidden_state=(N               'input_mask[0][0]',             \n",
      "                                one, 512, 768),                   'segment_ids[0][0]']            \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " tf.tile_2 (TFOpLambda)         (None, 512, 512)     0           ['tf.expand_dims_2[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 512, 1280)    0           ['tf_conv_bert_model[1][0]',     \n",
      "                                                                  'tf.tile_2[0][0]']              \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  (None, 512, 512)    3147776     ['concatenate_1[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_3 (Bidirectional  (None, 256)         656384      ['bidirectional_2[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 128)          32896       ['bidirectional_3[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_78 (Dropout)           (None, 128)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 4)            516         ['dropout_78[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,521,676\n",
      "Trainable params: 109,521,676\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "convBert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, min_lr=0.000001)\n",
    "\n",
    "train_sh = convBert_model.fit(\n",
    "    albert_train_input, y_train,\n",
    "    validation_data=(albert_validation_input, y_validation),\n",
    "    epochs=35,\n",
    "    callbacks=[earlystopping, reduce_lr],\n",
    "    batch_size=8,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = convBert_model.evaluate(albert_test_input, test_labels, verbose=0)\n",
    "\n",
    "\n",
    "print('Test accuracy:', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "predicted_labels = convBert_model.predict(albert_test_input)\n",
    "\n",
    "\n",
    "predicted_labels = np.argmax(predicted_labels, axis=1)\n",
    "\n",
    "test_labels_decoded = np.argmax(test_labels, axis=1)\n",
    "\n",
    "print(classification_report(test_labels_decoded, predicted_labels))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4278154,
     "sourceId": 7364364,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4278713,
     "sourceId": 7365200,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
